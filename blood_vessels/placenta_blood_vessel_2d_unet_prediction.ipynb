{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ This notebook relies on version 1 of the fastai library. If Google Colab or your local python environment upgrade to version 2, it will no longer work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twelve way segmentation workbook\n",
    "- Takes an input data volume and a 2D Unet trained for binary segmentation - our trained models and some test data can be downloaded from Zenodo [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4249627.svg)](https://doi.org/10.5281/zenodo.4249627)\n",
    "- Slices the data volume in the three orthogonal planes and predicts output for each slice\n",
    "- The predictions are recombined into 3D volumes and then summed\n",
    "- The input data volume is rotated by 90 degrees before the slicing and prediction steps are performed again\n",
    "- This is repeated until 4 rotations have been been performed\n",
    "- All the volumes are summed to give a prediction that is the sum of predictions in 12 different directions, a list of threshold values for a consensus cutoff is used to give a number of output volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important for Colab use\n",
    "If running this notebook on Google Colab, please go to the 'runtime' menu then 'change runtime type' and select 'GPU' as the 'Hardware Accelerator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "import re\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import h5py as h5\n",
    "from fastai.vision import *\n",
    "from skimage import img_as_ubyte, io, exposure, img_as_float\n",
    "from skimage.transform import resize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Google Colab, run this cell to download the Unet and the data for prediction from Zenodo\n",
    "!mkdir -p prediction_data/\n",
    "!curl  -o prediction_data/specimen1_512cube_zyx_800-1312_1000-1512_700-1212_DATA.h5 https://zenodo.org/api/files/fc8e12d1-4256-4ed9-8a23-66c0d6c64379/specimen1_512cube_zyx_800-1312_1000-1512_700-1212_DATA.h5\n",
    "!curl  -o prediction_data/specimen1_placental_blood_vessels_2dUnet.pkl https://zenodo.org/api/files/fc8e12d1-4256-4ed9-8a23-66c0d6c64379/specimen1_placental_blood_vessels_2dUnet.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Paths that are needed\n",
    "1. `root_path` - Root filepath for output directories, folder will be created\n",
    "2. `data_vol_path` - Path to the HDF5 volume to be segmented. Data should be in `/data` inside the file\n",
    "3. `learner_root_path` - Path to the folder containing the model file\n",
    "4. `learner_file` - Filename of the pickled 2d Unet model file. Needs to have been trained using BCE loss. For binary segmentation only\n",
    "5. `consensus_vals` - List of consensus cutoff values for agreement between volumes e.g. if 10 is in the list a volume will be output thresholded on consensus between 10 volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('data_output')\n",
    "data_vol_path = Path('prediction_data/specimen1_512cube_zyx_800-1312_1000-1512_700-1212_DATA.h5')\n",
    "learner_root_path = Path('prediction_data')\n",
    "learner_file = 'specimen1_placental_blood_vessels_2dUnet.pkl'\n",
    "consensus_vals = [8, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs = partial(os.makedirs, exist_ok=True)\n",
    "\n",
    "def da_from_data(path):\n",
    "    \"\"\"Returns a dask array when given a path to an HDF5 file.\n",
    "    \n",
    "    The data is assumed to be found in '/data' in the file.\n",
    "    \n",
    "    Args:\n",
    "        path (Path): The path to the HDF5 file.\n",
    "\n",
    "    Returns:\n",
    "        dask.array: A dask array object for the data stored in the HDF5 file.\"\"\"\n",
    "\n",
    "    f = h5.File(path, 'r')\n",
    "    d = f['/data']\n",
    "    return da.from_array(d, chunks='auto')\n",
    "\n",
    "# Needed because prediction doesn't work on odd sized images\n",
    "def fix_odd_sides(example_image):\n",
    "    \"\"\"Replaces an an odd image dimension with an even dimension by padding.\n",
    "    \n",
    "    Taken from https://forums.fast.ai/t/segmentation-mask-prediction-on-different-input-image-sizes/44389/7.\"\"\"\n",
    "    if (list(example_image.size)[0] % 2) != 0:\n",
    "        example_image = crop_pad(example_image, \n",
    "                            size=(list(example_image.size)[0]+1, list(example_image.size)[1]),\n",
    "                            padding_mode = 'reflection')\n",
    "\n",
    "    if (list(example_image.size)[1] % 2) != 0:\n",
    "        example_image = crop_pad(example_image, \n",
    "                            size=(list(example_image.size)[0], list(example_image.size)[1] + 1),\n",
    "                            padding_mode = 'reflection')\n",
    "\n",
    "def predict_single_slice(learn, axis, val, data, output_path):\n",
    "    \"\"\"Takes in a 2d data array and saves the predicted U-net segmentation to disk.\n",
    "    \n",
    "    Args:\n",
    "        learn (fastai.vision.learner): The trained 2d U-net model.\n",
    "        axis (str): The name of the axis to incorporate in the output filename.\n",
    "        val (int): The slice number to incorporate in the output filename.\n",
    "        data (numpy.array): The 2d data array to be fed into the U-net.\n",
    "        output_path (Path): The path to directory for file output.\"\"\"\n",
    "    data = img_as_float(data)\n",
    "    img = Image(pil2tensor(data, dtype=np.float32))\n",
    "    fix_odd_sides(img)\n",
    "    prediction = learn.predict(img)\n",
    "    pred_slice = img_as_ubyte(prediction[1][0])\n",
    "    io.imsave(output_path/f\"unet_prediction_{axis}_stack_{val}.png\", pred_slice)\n",
    "\n",
    "def predict_orthog_slices_to_disk(learn, axis, data_arr, output_path):\n",
    "    \"\"\"Coordinates the 2d U-net prediction of an image volume in one or all of the three orthogonal\n",
    "    planes to images on disk. \n",
    "    \n",
    "    Args:\n",
    "        learn (fastai.vision.learner): The trained 2d U-net model.\n",
    "        axis (string): Which plane to slice the data in. Either 'x', 'y, 'z' or 'all'.\n",
    "        data_array (array): The data volume to be sliced and predicted.\n",
    "        output_path (Path): A Path object to the output directory.\"\"\"\n",
    "    data_shape = data_arr.shape\n",
    "    name_prefix = 'seg'\n",
    "    # There has to be a cleverer way to do this!\n",
    "    if axis in ['z', 'all']:\n",
    "        print('Predicting z stack')\n",
    "        for val in range(data_shape[0]):\n",
    "            predict_single_slice(learn, 'z', val, data_arr[val, :, :], output_path)\n",
    "    if axis in ['x', 'all']:\n",
    "        print('Predicting x stack')\n",
    "        for val in range(data_shape[1]):\n",
    "            predict_single_slice(learn, 'x', val, data_arr[:, val, :], output_path)                    \n",
    "    if axis in ['y', 'all']:\n",
    "        print('Predicting y stack')\n",
    "        for val in range(data_shape[2]):\n",
    "            predict_single_slice(learn, 'y', val, data_arr[:, :, val], output_path)\n",
    "    if axis not in ['x', 'y', 'z', 'all']:\n",
    "        print(\"Axis should be one of: [all, x, y, or z]!\")\n",
    "\n",
    "def setup_folder_stucture(root_path):\n",
    "    \"\"\"Sets up a folder structure to store the predicted images.\n",
    "    \n",
    "    Args:\n",
    "        root_path (Path): The top level directory for data output.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing a key and the corresponding path for a folder.\"\"\"\n",
    "    non_rotated = root_path/f'{date.today()}_non_rotated_seg_slices'\n",
    "    rot_90_seg = root_path/f'{date.today()}_rot_90_seg_slices'\n",
    "    rot_180_seg = root_path/f'{date.today()}_rot_180_seg_slices'\n",
    "    rot_270_seg = root_path/f'{date.today()}_rot_270_seg_slices'\n",
    "    \n",
    "    dir_list = [\n",
    "        ('non_rotated', non_rotated),\n",
    "        ('rot_90_seg', rot_90_seg),\n",
    "        ('rot_180_seg', rot_180_seg),\n",
    "        ('rot_270_seg', rot_270_seg)\n",
    "    ]\n",
    "    for key, dir_path in dir_list:\n",
    "        makedirs(dir_path)\n",
    "    return dir_list\n",
    "\n",
    "# Need the loss in order to load the learner..\n",
    "def bce_loss(logits, labels):\n",
    "    \"\"\"Defines the binary cross entropy loss function used when training a U-net on binary data.\"\"\"\n",
    "    logits=logits[:,1,:,:].float()\n",
    "    labels = labels.squeeze(1).float()\n",
    "    return F.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "# Since we're doing binary segmentation, we need to define a binary label list class\n",
    "class BinaryLabelList(SegmentationLabelList):\n",
    "    def open(self, fn): return open_mask(fn)\n",
    "\n",
    "class BinaryItemList(SegmentationItemList):\n",
    "    _label_cls = BinaryLabelList\n",
    "\n",
    "def combine_slices_to_vol(folder_path):\n",
    "    \"\"\"Combines the orthogonally sliced png images in a folder to HDF5 volumes with a common orientation.\"\"\"\n",
    "    output_path_list = []\n",
    "    file_list = folder_path.ls()\n",
    "    axis_list = ['z', 'x', 'y']\n",
    "    axis_regex = re.compile(r'\\_(\\D)\\_')\n",
    "    number_regex = re.compile(r'\\_(\\d+)\\.png')\n",
    "    for axis in axis_list:\n",
    "        axis_files = [x for x in file_list if re.search(f'\\_({axis})\\_', str(x))]\n",
    "        print(f\"Creating volume from {axis} stack\")\n",
    "        print(f'{len(axis_files)} files found')\n",
    "        first_im = open_image(axis_files[0])\n",
    "        shape_tuple = first_im.shape\n",
    "        z_dim = len(axis_files)\n",
    "        x_dim = shape_tuple[1]\n",
    "        y_dim = shape_tuple[2]\n",
    "        data_vol = np.empty([z_dim, x_dim, y_dim], dtype=np.uint8)\n",
    "        for filename in axis_files:\n",
    "            m = number_regex.search(str(filename))\n",
    "            pos = int(m.group(1))\n",
    "            im_data = io.imread(filename)\n",
    "            data_vol[pos, :, :] = im_data\n",
    "        if axis == 'x':\n",
    "            data_vol = np.swapaxes(data_vol, 0, 1)\n",
    "        if axis == 'y':\n",
    "            data_vol = np.swapaxes(data_vol, 0, 2)\n",
    "            data_vol = np.swapaxes(data_vol, 0, 1)\n",
    "        output_path = folder_path/f'{axis}_axis_seg_combined.h5'\n",
    "        output_path_list.append(output_path)\n",
    "        print(f'Outputting volume to {output_path}')\n",
    "        with h5.File(output_path, 'w') as f:\n",
    "            f['/data'] = data_vol\n",
    "        # Delete the images\n",
    "        print(f\"Deleting {len(axis_files)} image files for axis {axis}\")\n",
    "        for filename in axis_files:\n",
    "            os.remove(filename)\n",
    "    return output_path_list\n",
    "\n",
    "def combine_vols(output_path_list, k, prefix, final=False):\n",
    "    \"\"\"Sums volumes to give a combination of binary segmentations and saves to disk.\"\"\"\n",
    "    num_vols = len(output_path_list)\n",
    "    combined = da_from_data(output_path_list[0])\n",
    "    for subsequent in output_path_list[1:]:\n",
    "        combined += da_from_data(subsequent)\n",
    "    combined_out_path = output_path_list[0].parent.parent/f'{date.today()}_{prefix}_{num_vols}_volumes_combined.h5'\n",
    "    if final:\n",
    "        combined_out_path = output_path_list[0].parent/f'{date.today()}_{prefix}_{num_vols}_volumes_combined.h5'\n",
    "    print(f'Outputting the {num_vols} combined volumes to {combined_out_path}')\n",
    "    combined = combined.compute()\n",
    "    combined = np.rot90(combined, k, (1, 0))\n",
    "    with h5.File(combined_out_path, 'w') as f:\n",
    "        f['/data'] = combined\n",
    "    return combined_out_path\n",
    "\n",
    "def threshold(input_path, range_list):\n",
    "    \"\"\"Outputs a consensus thresholded volume from combination of binary volumes.\"\"\"\n",
    "    for val in range_list:\n",
    "        combined = da_from_data(input_path)\n",
    "        combined_out = input_path.parent/f'{date.today()}_combined_thresh_cutoff_{val}.h5'\n",
    "        combined[combined < val] = 0\n",
    "        combined[combined >= val] = 255\n",
    "        print(f'Writing to {combined_out}')\n",
    "        combined.to_hdf5(combined_out, '/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a root directory for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data volume and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr = da_from_data(data_vol_path)\n",
    "data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(learner_root_path, learner_file)\n",
    "# Remove the restriction on the model prediction size\n",
    "learn.data.single_ds.tfmargs['size'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the loop to do repeated prediction and recombination steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final combined output file of 12 summed volumes will be found in the `data_output` directory with a filename `YYYY-MM_DD_final_4_volumes_combined` and the files with a consensus threshold applied will be found in file with filenames `YYYY-MM_DD_combined_thresh_cutoff_X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 'all'\n",
    "dir_list = setup_folder_stucture(root_path)\n",
    "combined_vol_paths = []\n",
    "data_arr = data_arr.compute()\n",
    "for k in range(4):\n",
    "    key, output_path = dir_list[k]\n",
    "    print(f'Key : {key}, output : {output_path}')\n",
    "    print(f'Rotating volume {k * 90} degrees')\n",
    "    rotated = np.rot90(data_arr, k)\n",
    "    predict_orthog_slices_to_disk(learn, axis, rotated, output_path)\n",
    "    output_path_list = combine_slices_to_vol(output_path)\n",
    "    fp = combine_vols(output_path_list, k, key)\n",
    "    combined_vol_paths.append(fp)\n",
    "# Combine all the volumes\n",
    "final_combined = combine_vols(combined_vol_paths, 0, 'final', True)\n",
    "threshold(final_combined, consensus_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
